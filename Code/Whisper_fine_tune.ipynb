{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face Blog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahdj\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'transcript', 'summary'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'transcript', 'summary'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'transcript', 'summary'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Define paths to your data folders\n",
    "audio_folder = \"../Dataset/audio\"\n",
    "transcripts_folder = \"../Dataset/text\"\n",
    "summaries_folder = \"../Dataset/summary\"\n",
    "\n",
    "# Create a list of audio, transcript, and summary file paths\n",
    "audio_files = sorted([os.path.join(audio_folder, filename) for filename in os.listdir(audio_folder)])\n",
    "transcript_files = sorted([os.path.join(transcripts_folder, filename) for filename in os.listdir(transcripts_folder)])\n",
    "summary_files = sorted([os.path.join(summaries_folder, filename) for filename in os.listdir(summaries_folder)])\n",
    "\n",
    "# Check if the number of audio, transcript, and summary files match\n",
    "if len(audio_files) != len(transcript_files) != len(summary_files):\n",
    "    raise ValueError(\"Number of files in each folder must match\")\n",
    "\n",
    "# Create a dictionary containing your dataset\n",
    "dataset_dict = {\n",
    "    \"audio\": audio_files,\n",
    "    \"transcript\": transcript_files,\n",
    "    \"summary\": summary_files,\n",
    "}\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "custom_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_percentage = 0.8\n",
    "validation_percentage = 0.1\n",
    "test_percentage = 0.1\n",
    "\n",
    "train_dataset = custom_dataset.select(range(int(len(custom_dataset) * train_percentage)))\n",
    "\n",
    "validation_dataset = custom_dataset.select(range(int(len(custom_dataset) * train_percentage), \n",
    "                                                 int(len(custom_dataset) * (train_percentage + validation_percentage))))\n",
    "\n",
    "test_dataset = custom_dataset.select(range(int(len(custom_dataset) * (train_percentage + validation_percentage)), \n",
    "                                           len(custom_dataset)))\n",
    "\n",
    "# Create a DatasetDict\n",
    "custom_dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n",
    "\n",
    "# Print the dataset\n",
    "print(custom_dataset_dict)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper=\"openai/whisper-small.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(whisper, \n",
    "                                             language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(whisper, language=\"English\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': '../Dataset/audio\\\\000000.mp3', 'transcript': '../Dataset/text\\\\000000.txt', 'summary': '../Dataset/summary\\\\000000.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(custom_dataset_dict[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "custom_dataset_dict = custom_dataset_dict.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], \n",
    "                                sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcript\"]).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=14): 100%|██████████| 400/400 [06:39<00:00,  1.00 examples/s]\n",
      "Map (num_proc=14): 100%|██████████| 50/50 [00:52<00:00,  1.06s/ examples]\n",
      "Map (num_proc=14): 100%|██████████| 50/50 [01:07<00:00,  1.36s/ examples]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Define the prepare_dataset function with the feature_extractor argument\n",
    "def prepare_dataset(batch, feature_extractor,tokenizer):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcript\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# Use partial to create a function with feature_extractor as a fixed argument\n",
    "prepare_with_feature_extractor = partial(prepare_dataset, feature_extractor=feature_extractor,tokenizer=tokenizer)\n",
    "\n",
    "# Pass the prepared function when using .map()\n",
    "custom_dataset_dict = custom_dataset_dict.map(\n",
    "    prepare_with_feature_extractor,\n",
    "    remove_columns=custom_dataset_dict.column_names[\"train\"],\n",
    "    num_proc=14\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 80, 3000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.array(custom_dataset_dict[\"train\"]['input_features']).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Colloctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        \n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # If available, set the device to GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# The rest of your code remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-en-sumerizer\",  \n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    learning_rate=1e-5,\n",
    "    #warmup_steps=500,\n",
    "    max_steps=1000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=5,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=20,\n",
    "    save_steps=250,\n",
    "    eval_steps=125,\n",
    "    logging_steps=2,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=custom_dataset_dict[\"train\"],\n",
    "    eval_dataset=custom_dataset_dict[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 882/1000 [23:33:23<3:18:24, 100.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.21e-06, 'epoch': 35.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 884/1000 [23:36:36<3:11:14, 98.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.19e-06, 'epoch': 35.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 886/1000 [23:39:51<3:06:37, 98.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.1700000000000002e-06, 'epoch': 35.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 888/1000 [23:42:58<2:58:52, 95.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.1500000000000002e-06, 'epoch': 35.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 890/1000 [23:46:05<2:53:27, 94.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.1300000000000002e-06, 'epoch': 35.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 892/1000 [23:49:12<2:49:15, 94.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.1100000000000002e-06, 'epoch': 35.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 894/1000 [23:52:25<2:48:50, 95.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.0900000000000002e-06, 'epoch': 35.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 896/1000 [23:55:45<2:49:38, 97.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.0700000000000001e-06, 'epoch': 35.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 898/1000 [23:59:05<2:48:21, 99.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.0500000000000001e-06, 'epoch': 35.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 900/1000 [24:02:24<2:45:20, 99.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.03e-06, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 901/1000 [24:04:05<2:44:48, 99.88s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\mahdj\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mahdj\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1836\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1837\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1840\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1841\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1843\u001b[0m ):\n\u001b[0;32m   1844\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\mahdj\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\trainer.py:2693\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2691\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m   2692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2693\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[0;32m   2695\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\mahdj\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\accelerate\\accelerator.py:1921\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1919\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1920\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1921\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1922\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1923\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mahdj\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mahdj\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  48957,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  16,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  17,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  18,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  19,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  20,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  21,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  22,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  23,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  8465,\n",
       "  24,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  19,\n",
       "  4550,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  16169,\n",
       "  16,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  16169,\n",
       "  17,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  16169,\n",
       "  18,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  16169,\n",
       "  19,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  16169,\n",
       "  20,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  19,\n",
       "  15237,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  16169,\n",
       "  22,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  16169,\n",
       "  23,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  16169,\n",
       "  24,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  19,\n",
       "  5867,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14060,\n",
       "  16,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14060,\n",
       "  17,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14060,\n",
       "  18,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14060,\n",
       "  19,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  19,\n",
       "  11901,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14060,\n",
       "  21,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14060,\n",
       "  22,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14060,\n",
       "  23,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14060,\n",
       "  24,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  19,\n",
       "  4702,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  16,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  17,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  18,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  19,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  20,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  21,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  22,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  23,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  13318,\n",
       "  24,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  19,\n",
       "  7771,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14938,\n",
       "  16,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14938,\n",
       "  17,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14938,\n",
       "  18,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14938,\n",
       "  19,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14938,\n",
       "  20,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14938,\n",
       "  21,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14938,\n",
       "  22,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  14938,\n",
       "  23,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257],\n",
       " [50258,\n",
       "  50259,\n",
       "  50363,\n",
       "  353,\n",
       "  14,\n",
       "  35,\n",
       "  37892,\n",
       "  302,\n",
       "  14,\n",
       "  25111,\n",
       "  59,\n",
       "  1360,\n",
       "  19,\n",
       "  8494,\n",
       "  13,\n",
       "  83,\n",
       "  734,\n",
       "  50257]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset_dict[\"test\"][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 17 at dim 1 (got 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m loss_fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     14\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(predictions)  \u001b[39m# Convert predictions to a PyTorch tensor\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m references \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(references)  \u001b[39m# Convert references to a PyTorch tensor\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mvocab_size), references\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[39m# Print the results\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 17 at dim 1 (got 18)"
     ]
    }
   ],
   "source": [
    "from datasets import  load_metric\n",
    "# Generate predictions on the test dataset\n",
    "predictions = trainer.predict(custom_dataset_dict[\"test\"])\n",
    "\n",
    "# Calculate the WER for each prediction compared to the ground truth\n",
    "metric = load_metric(\"wer\")  # Load the WER metric\n",
    "predictions = predictions.predictions\n",
    "references = custom_dataset_dict[\"test\"][\"labels\"]  # Replace with the correct column name\n",
    "\n",
    "wer = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Calculate the loss on the test dataset\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "logits = torch.tensor(predictions)  # Convert predictions to a PyTorch tensor\n",
    "references = torch.tensor(references)  # Convert references to a PyTorch tensor\n",
    "loss = loss_fn(logits.view(-1, model.config.vocab_size), references.view(-1))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Word Error Rate (WER): {wer}\")\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Plot the results (you can use your preferred plotting library)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot WER\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(wer, label=\"WER\")\n",
    "plt.xlabel(\"Example Index\")\n",
    "plt.ylabel(\"WER\")\n",
    "plt.title(\"Word Error Rate (WER) on Test Dataset\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = load_dataset(\"your_dataset_name\", \"test\")\n",
    "\n",
    "# Initialize a data collator for evaluation\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Create a trainer for evaluation\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Change to your desired output directory\n",
    "    per_device_eval_batch_size=4,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Calculate the WER for each prediction compared to the ground truth\n",
    "metric = load_metric(\"wer\")  # Load the WER metric\n",
    "predictions = predictions.predictions\n",
    "references = test_dataset[\"your_target_column_name\"]  # Replace with the correct column name\n",
    "\n",
    "wer = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Calculate the loss on the test dataset\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "logits = torch.tensor(predictions)  # Convert predictions to a PyTorch tensor\n",
    "references = torch.tensor(references)  # Convert references to a PyTorch tensor\n",
    "loss = loss_fn(logits.view(-1, model.config.vocab_size), references.view(-1))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Word Error Rate (WER): {wer}\")\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Plot the results (you can use your preferred plotting library)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot WER\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(wer, label=\"WER\")\n",
    "plt.xlabel(\"Example Index\")\n",
    "plt.ylabel(\"WER\")\n",
    "plt.title(\"Word Error Rate (WER) on Test Dataset\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
